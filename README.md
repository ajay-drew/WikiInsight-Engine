# WikiInsight Engine

An AI-driven system that uses **embeddings + clustering** to group Wikipedia articles into intuitive **topic clusters**, with a dashboard to explore clusters, top pages, and cluster keywordsâ€”no labels, just unsupervised structure.

## ğŸ¯ Problem

Wikipedia is huge and fragmented. Itâ€™s hard for editors and readers to see the **big-picture structure** of topics, discover related articles, or identify clusters that are thin, noisy, or underdeveloped. This project builds an unsupervised topic map of Wikipedia so you can explore how articles group together in practice.

## ğŸš€ MVP Features (3-4 Weeks)

- **Topic Clustering**: Embeddings + clustering (e.g., KMeans/HDBSCAN) to group articles into coherent topic clusters
- **Network Analysis**: Article similarity networks, community detection, centrality metrics (not full GNN)
- **Cluster Explainability**: Simple keyword extraction and SHAP-style feature importance to describe clusters and top articles
- **MLOps Pipeline**: DVC versioning, MLflow tracking, Prefect orchestration, monitoring
- **Interactive Frontend**: React + Vite + Tailwind UI with cluster explorer and summaries
- **REST API**: FastAPI-based service for programmatic access to clusters and nearest-neighbor queries

## ğŸ“‹ Requirements

- Python 3.9+
- See `requirements.txt` for full dependencies
- **Locally runnable** - no cloud deployment needed

## ğŸ› ï¸ Installation

```bash
# Clone the repository
git clone <repository-url>
cd WikiInsight-Engine

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Download spaCy model
python -m spacy download en_core_web_sm

# Install in development mode
pip install -e .
```

## ğŸ—ï¸ Project Structure

```
wikiinsight-engine/
â”œâ”€â”€ data/              # Data storage (DVC tracked)
â”‚   â”œâ”€â”€ raw/           # Raw Wikipedia data
â”‚   â”œâ”€â”€ processed/     # Processed articles
â”‚   â””â”€â”€ features/      # Engineered features
â”œâ”€â”€ models/            # Trained models (MLflow registry)
â”‚   â”œâ”€â”€ xgboost/
â”‚   â”œâ”€â”€ lightgbm/
â”‚   â”œâ”€â”€ logistic/
â”‚   â””â”€â”€ ensemble/
â”œâ”€â”€ src/               # Source code
â”‚   â”œâ”€â”€ ingestion/     # Wikipedia/Wikidata API clients
â”‚   â”œâ”€â”€ preprocessing/ # Text cleaning, embeddings, graph builders
â”‚   â”œâ”€â”€ modeling/      # Clustering pipeline + topic index
â”‚   â”œâ”€â”€ explainability/# SHAP explainers (optional)
â”‚   â””â”€â”€ api/           # FastAPI application
â”œâ”€â”€ frontend/          # React + Vite + Tailwind frontend
â”œâ”€â”€ pipelines/         # Prefect workflows
â”‚   â”œâ”€â”€ flow_ingestion.py
â”‚   â”œâ”€â”€ flow_training.py
â”‚   â””â”€â”€ flow_inference.py
â”œâ”€â”€ tests/             # Test suite
â”œâ”€â”€ monitoring/        # Monitoring dashboard
â””â”€â”€ notebooks/         # Jupyter notebooks for exploration
```

## ğŸš¦ Quick Start

### Setup MLOps Tools

```bash
# Initialize DVC
dvc init

# Start MLflow tracking server
mlflow ui --port 5000

# Start Prefect server (optional)
prefect server start
```

### Data Ingestion

```bash
python -m src.ingestion.fetch_wikipedia_data
```

### Run API (backend)

```bash
uvicorn src.api.main:app --reload
```

or use the convenience script on Windows:

```bash
run_app.cmd
```

This will start the FastAPI backend and the React/Vite frontend together (Vite dev server on port 5173 by default).

### Run Frontend (manually)

```bash
cd frontend
npm install    # or pnpm install / yarn
npm run dev   # starts Vite dev server on http://localhost:5173
```

## ğŸ“Š MVP Timeline (3-4 Weeks)

**Week 1**: Data + Features (DVC, MLflow, Prefect setup â†’ ingestion â†’ embeddings + basic features)

**Week 2**: Clustering + Evaluation (train clustering model(s) â†’ cluster labeling/keywords â†’ basic quality checks)

**Week 3**: Visualization + Orchestration (network graphs â†’ cluster explorer dashboard â†’ Prefect flows)

**Week 4**: MLOps + Polish (monitoring â†’ CI/CD â†’ testing â†’ documentation)

## ğŸ¯ Success Metrics

- High qualitative coherence of topic clusters (manual inspection)
- Ability to retrieve top-N similar articles for a given page in <1s
- Network graph renders 20K+ nodes in <5 seconds
- MLflow tracks 50+ experiments
- DVC versions 5+ datasets
- Prefect runs 3 flows end-to-end
- CI/CD passes 90%+ test coverage

## ğŸ”§ Tech Stack

**Core ML**: `scikit-learn`, `XGBoost`, `LightGBM`, `sentence-transformers`, `spaCy`, `shap`

**Graph Analysis**: `NetworkX`, `igraph`, `pyvis`

**MLOps**: `MLflow`, `DVC`, `Prefect`, `pytest`, `Great Expectations`

**Visualization**: React + Vite + Tailwind (frontend), `Plotly`, `matplotlib` (for future plots)

**Data**: `pandas`, `pyarrow`, `SQLite3`, `mwclient`

## ğŸ¤ Contributing

Contributions welcome! Please read our contributing guidelines.

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ”— Resources

- [Wikipedia API Documentation](https://www.mediawiki.org/wiki/API:Main_page)
- [Wikidata API](https://www.wikidata.org/wiki/Wikidata:Main_Page)
- [DVC Documentation](https://dvc.org/)
- [MLflow Documentation](https://mlflow.org/)
- [Prefect Documentation](https://www.prefect.io/)
- **Note**: Planning documents (project description, implementation analysis, phase breakdowns) are in the `phases/` folder (gitignored)
